{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np,sys,time\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets\n",
    "import os\n",
    "import input_data\n",
    "import numpy as np,sys\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLu(x):\n",
    "    mask = (x>0) * 1.0\n",
    "    return mask *x\n",
    "def d_ReLu(x):\n",
    "    mask = (x>0) * 1.0\n",
    "    return mask \n",
    "\n",
    "def Lrelu(x):\n",
    "#    alpha = 0.0001\n",
    "#    return np.where(x > 0, x, x * alpha) \n",
    "    mask = (x>0) * 1.0\n",
    "    return mask *x\n",
    "def d_Lrelu(x):\n",
    "    mask = (x>0) * 1.0\n",
    "    return mask \n",
    "    alpha = 0.0001\n",
    "    return  - alpha * (x<0) +  (x>=0) * 1\n",
    "def arctan(x):\n",
    "    return np.arctan(x)\n",
    "def d_arctan(x):\n",
    "    return 1 / (1 + x ** 2)\n",
    "\n",
    "def log(x):\n",
    "    return 1 / ( 1+ np.exp(-1*x))\n",
    "def d_log(x):\n",
    "    return log(x) * (1 - log(x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "def d_tanh(x):\n",
    "    return 1 - np.tanh(x) ** 2\n",
    "\n",
    "def plot(samples, title):\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = gridspec.GridSpec(4, 4)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.title(title)\n",
    "        plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please Input a Random Number to Seed3\n"
     ]
    }
   ],
   "source": [
    "random_numer = int(input(\"Please Input a Random Number to Seed\"))\n",
    "np.random.seed(random_numer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Load Data ----------\n",
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# 1. Load Data and declare hyper\n",
    "print('--------- Load Data ----------')\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=False)\n",
    "temp = mnist.test\n",
    "images, labels = temp.images, temp.labels\n",
    "images, labels = shuffle(np.asarray(images),np.asarray(labels))\n",
    "num_epoch = 50\n",
    "learing_rate = 0.00009\n",
    "G_input = 100\n",
    "hidden_input,hidden_input2,hidden_input3 = 128,256,346\n",
    "hidden_input4,hidden_input5,hidden_input6 = 480,560,686\n",
    "hidden_input7,hidden_input8,hidden_input9 = 800,1020,1400\n",
    "hidden_input10 = 1800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Declare Hyper Parameters ----------\n"
     ]
    }
   ],
   "source": [
    "print('--------- Declare Hyper Parameters ----------')\n",
    "# 2. Declare Weights\n",
    "D_W1 = np.random.normal(size=(784,hidden_input),scale=(1. / np.sqrt(784 / 2.)))   *0.002\n",
    "# D_b1 = np.random.normal(size=(128),scale=(1. / np.sqrt(128 / 2.)))       *0.002\n",
    "D_b1 = np.zeros(hidden_input)\n",
    "\n",
    "D_W2 = np.random.normal(size=(hidden_input,hidden_input2),scale=(1. / np.sqrt(hidden_input / 2.)))   *0.002\n",
    "# G_b1 = np.random.normal(size=(128),scale=(1. / np.sqrt(128 / 2.)))      *0.002\n",
    "D_b2 = np.zeros(hidden_input2)\n",
    "\n",
    "D_W3 = np.random.normal(size=(hidden_input2,hidden_input3),scale=(1. / np.sqrt(hidden_input2 / 2.)))   *0.002\n",
    "# G_b1 = np.random.normal(size=(128),scale=(1. / np.sqrt(128 / 2.)))      *0.002\n",
    "D_b3 = np.zeros(hidden_input3)\n",
    "\n",
    "D_W4 = np.random.normal(size=(hidden_input3,1),scale=(1. / np.sqrt(hidden_input / 2.)))     *0.002\n",
    "# D_b2 = np.random.normal(size=(1),scale=(1. / np.sqrt(1 / 2.)))           *0.002\n",
    "D_b4 = np.zeros(1)\n",
    "\n",
    "\n",
    "G_W1 = np.random.normal(size=(G_input,hidden_input),scale=(1. / np.sqrt(G_input / 2.)))   *0.002\n",
    "# G_b1 = np.random.normal(size=(128),scale=(1. / np.sqrt(128 / 2.)))      *0.002\n",
    "G_b1 = np.zeros(hidden_input)\n",
    "\n",
    "G_W2 = np.random.normal(size=(hidden_input,hidden_input2),scale=(1. / np.sqrt(hidden_input / 2.)))   *0.002\n",
    "# G_b1 = np.random.normal(size=(128),scale=(1. / np.sqrt(128 / 2.)))      *0.002\n",
    "G_b2 = np.zeros(hidden_input2)\n",
    "\n",
    "G_W3 = np.random.normal(size=(hidden_input2,hidden_input3),scale=(1. / np.sqrt(hidden_input2 / 2.)))   *0.002\n",
    "# G_b1 = np.random.normal(size=(128),scale=(1. / np.sqrt(128 / 2.)))      *0.002\n",
    "G_b3 = np.zeros(hidden_input3)\n",
    "\n",
    "G_W4 = np.random.normal(size=(hidden_input3,hidden_input4),scale=(1. / np.sqrt(hidden_input3 / 2.)))   *0.002\n",
    "# G_b1 = np.random.normal(size=(128),scale=(1. / np.sqrt(128 / 2.)))      *0.002\n",
    "G_b4 = np.zeros(hidden_input4)\n",
    "\n",
    "G_W5 = np.random.normal(size=(hidden_input4,hidden_input5),scale=(1. / np.sqrt(hidden_input4 / 2.)))   *0.002\n",
    "# G_b1 = np.random.normal(size=(128),scale=(1. / np.sqrt(128 / 2.)))      *0.002\n",
    "G_b5 = np.zeros(hidden_input5)\n",
    "\n",
    "G_W6 = np.random.normal(size=(hidden_input5,hidden_input6),scale=(1. / np.sqrt(hidden_input5 / 2.)))   *0.002\n",
    "# G_b1 = np.random.normal(size=(128),scale=(1. / np.sqrt(128 / 2.)))      *0.002\n",
    "G_b6 = np.zeros(hidden_input6)\n",
    "\n",
    "G_W7 = np.random.normal(size=(hidden_input6,hidden_input7),scale=(1. / np.sqrt(hidden_input5 / 2.)))   *0.002\n",
    "# G_b1 = np.random.normal(size=(128),scale=(1. / np.sqrt(128 / 2.)))      *0.002\n",
    "G_b7 = np.zeros(hidden_input7)\n",
    "\n",
    "G_W8 = np.random.normal(size=(hidden_input7,hidden_input8),scale=(1. / np.sqrt(hidden_input5 / 2.)))   *0.002\n",
    "# G_b1 = np.random.normal(size=(128),scale=(1. / np.sqrt(128 / 2.)))      *0.002\n",
    "G_b8 = np.zeros(hidden_input8)\n",
    "\n",
    "G_W9 = np.random.normal(size=(hidden_input8,hidden_input9),scale=(1. / np.sqrt(hidden_input5 / 2.)))   *0.002\n",
    "# G_b1 = np.random.normal(size=(128),scale=(1. / np.sqrt(128 / 2.)))      *0.002\n",
    "G_b9 = np.zeros(hidden_input9)\n",
    "\n",
    "G_W10 = np.random.normal(size=(hidden_input9,hidden_input10),scale=(1. / np.sqrt(hidden_input5 / 2.)))   *0.002\n",
    "# G_b1 = np.random.normal(size=(128),scale=(1. / np.sqrt(128 / 2.)))      *0.002\n",
    "G_b10 = np.zeros(hidden_input10)\n",
    "\n",
    "G_W11 = np.random.normal(size=(hidden_input10,784),scale=(1. / np.sqrt(hidden_input6 / 2.)))  *0.002\n",
    "# G_b2 = np.random.normal(size=(784),scale=(1. / np.sqrt(784 / 2.)))      *0.002\n",
    "G_b11 = np.zeros(784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. For Adam Optimzier\n",
    "m1w1,m1b1 = 0,0\n",
    "m2w2,m2b2 = 0,0\n",
    "m3w3,m3b3 = 0,0\n",
    "m4w4,m4b4 = 0,0\n",
    "\n",
    "v1w1,v1b1 = 0,0\n",
    "v2w2,v2b2 = 0,0\n",
    "v3w3,v3b3 = 0,0\n",
    "v4w4,v4b4 = 0,0\n",
    "\n",
    "v5,m5 = 0,0\n",
    "v6,m6 = 0,0\n",
    "v7,m7 = 0,0\n",
    "v8,m8 = 0,0\n",
    "v9,m9 = 0,0\n",
    "v10,m10 = 0,0\n",
    "v11,m11 = 0,0\n",
    "v12,m12 = 0,0\n",
    "v13,m13 = 0,0\n",
    "v14,m14 = 0,0\n",
    "v15,m15 = 0,0\n",
    "v16,m16 = 0,0\n",
    "v17,m17 = 0,0\n",
    "v19,m19 = 0,0\n",
    "v20,m20 = 0,0\n",
    "v21,m21 = 0,0\n",
    "v22,m22 = 0,0\n",
    "v23,m23 = 0,0\n",
    "v24,m24 = 0,0\n",
    "v25,m25 = 0,0\n",
    "v26,m26 = 0,0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_1,beta_2,eps = 0.9,0.999,0.00000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Started Training ----------\n",
      "Current Iter:  0  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  1  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  2  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  3  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  4  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  5  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  6  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  7  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  8  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  9  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  10  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  11  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  12  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  13  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  14  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  15  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  16  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  17  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  18  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  19  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  20  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  21  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  22  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  23  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  24  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  25  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  26  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  27  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  28  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  29  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  30  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  31  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  32  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  33  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  34  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  35  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  36  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  37  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  38  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  39  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  40  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  41  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  42  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  43  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  44  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  45  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  46  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  47  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  48  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  49  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  50  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  51  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  52  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  53  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  54  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  55  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  56  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  57  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  58  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  59  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  60  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  61  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  62  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  63  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  64  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  65  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  66  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  67  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  68  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  69  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  70  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  71  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  72  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  73  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  74  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  75  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  76  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  77  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  78  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  79  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  80  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  81  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  82  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  83  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  84  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  85  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  86  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  87  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  88  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  89  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  90  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  91  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  92  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  93  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  94  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  95  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  96  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  97  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n",
      "Current Iter:  98  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Iter:  99  Current D cost: [[1.38629438]]  Current G cost:  [[0.69329351]]\n"
     ]
    }
   ],
   "source": [
    "print('--------- Started Training ----------')\n",
    "for iter in range(num_epoch+50):\n",
    "    random_int = np.random.randint(len(images) - 5)\n",
    "    current_image = np.expand_dims(images[random_int],axis=0)\n",
    "    #fig = plot(current_image,\"REAL\")\n",
    "    # Func: Generate The first Fake Data\n",
    "    Z = np.random.uniform(-1., 1., size=[1, G_input])\n",
    "    Gl1 = Z.dot(G_W1) + G_b1\n",
    "    Gl1A = Lrelu(Gl1)\n",
    "    Gl2 = Gl1A.dot(G_W2) + G_b2\n",
    "    Gl2A = Lrelu(Gl2)\n",
    "    Gl3 = Gl2A.dot(G_W3) + G_b3\n",
    "    Gl3A = Lrelu(Gl3)\n",
    "    Gl4 = Gl3A.dot(G_W4) + G_b4\n",
    "    Gl4A = Lrelu(Gl4)\n",
    "    Gl5 = Gl4A.dot(G_W5) + G_b5\n",
    "    Gl5A = Lrelu(Gl5)\n",
    "    Gl6 = Gl5A.dot(G_W6) + G_b6\n",
    "    Gl6A = Lrelu(Gl6)\n",
    "    Gl7 = Gl6A.dot(G_W7) + G_b7\n",
    "    Gl7A = Lrelu(Gl7)\n",
    "    Gl8 = Gl7A.dot(G_W8) + G_b8\n",
    "    Gl8A = Lrelu(Gl8)\n",
    "    Gl9 = Gl8A.dot(G_W9) + G_b9\n",
    "    Gl9A = Lrelu(Gl9)\n",
    "    Gl10 = Gl9A.dot(G_W10) + G_b10\n",
    "    Gl10A = Lrelu(Gl10)\n",
    "    Gl11 = Gl10A.dot(G_W11) + G_b11\n",
    "\n",
    "    current_fake_data = log(Gl11)\n",
    "\n",
    "    # Func: Forward Feed for Real data\n",
    "    Dl1_r = current_image.dot(D_W1) + D_b1\n",
    "    Dl1_rA = Lrelu(Dl1_r)\n",
    "    Dl2_r = Dl1_rA.dot(D_W2) + D_b2\n",
    "    Dl2_rA = Lrelu(Dl2_r)\n",
    "    Dl3_r = Dl2_rA.dot(D_W3) + D_b3\n",
    "    Dl3_rA = Lrelu(Dl3_r)\n",
    "    Dl4_r = Dl3_rA.dot(D_W4) + D_b4\n",
    "    Dl4_rA = log(Dl4_r)\n",
    "\n",
    "    # Func: Forward Feed for Fake Data\n",
    "    Dl1_f = current_fake_data.dot(D_W1) + D_b1\n",
    "    Dl1_fA = Lrelu(Dl1_f)\n",
    "    Dl2_f = Dl1_fA.dot(D_W2) + D_b2\n",
    "    Dl2_fA = Lrelu(Dl2_f)\n",
    "    Dl3_f = Dl2_fA.dot(D_W3) + D_b3\n",
    "    Dl3_fA = Lrelu(Dl3_f)\n",
    "    Dl4_f = Dl3_fA.dot(D_W4) + D_b4\n",
    "    Dl4_fA = log(Dl4_f)\n",
    "\n",
    "\n",
    "    # Func: Cost D\n",
    "    D_cost = -(np.log(Dl4_rA) + np.log(1.0- Dl4_fA))\n",
    "\n",
    "    # Func: Gradient\n",
    "    grad_f_w4_part_1 =  1/(1.0- Dl4_fA)\n",
    "    grad_f_w4_part_2 =  d_log(Dl4_f)\n",
    "    grad_f_w4_part_3 =   Dl3_fA\n",
    "    grad_f_w4 =     grad_f_w4_part_3.T.dot(grad_f_w4_part_1 * grad_f_w4_part_2) \n",
    "    grad_f_b4 = grad_f_w4_part_1 * grad_f_w4_part_2\n",
    "\n",
    "    grad_f_w3_part_1 =  (grad_f_w4_part_1 * grad_f_w4_part_2).dot(D_W4.T)\n",
    "    grad_f_w3_part_2 =  d_ReLu(Dl3_f)\n",
    "    grad_f_w3_part_3 =   Dl2_fA\n",
    "    grad_f_w3 =       grad_f_w3_part_3.T.dot(grad_f_w3_part_1 * grad_f_w3_part_2) \n",
    "    grad_f_b3 =      grad_f_w3_part_1 * grad_f_w3_part_2\n",
    "    \n",
    "    grad_f_w2_part_1 = (grad_f_w3_part_1 * grad_f_w3_part_2).dot(D_W3.T)\n",
    "    grad_f_w2_part_2 = d_ReLu(Dl2_f)\n",
    "    grad_f_w2_part_3 = Dl1_fA\n",
    "    grad_f_w2 = grad_f_w2_part_3.T.dot(grad_f_w2_part_1 * grad_f_w2_part_2)\n",
    "    grad_f_b2 = (grad_f_w2_part_1 * grad_f_w2_part_2)\n",
    "\n",
    "    grad_f_w1_part_1 = (grad_f_w2_part_1 * grad_f_w2_part_2).dot(D_W2.T)\n",
    "    grad_f_w1_part_2 = d_ReLu(Dl1_f)\n",
    "    grad_f_w1_part_3 = current_fake_data\n",
    "    grad_f_w1 = grad_f_w1_part_3.T.dot(grad_f_w1_part_1 * grad_f_w1_part_2)\n",
    "    grad_f_b1 = grad_f_w1_part_1 * grad_f_w1_part_2\n",
    "    \n",
    "    #LATER\n",
    "    grad_r_w4_part_1 =  1/(1.0- Dl4_rA)\n",
    "    grad_r_w4_part_2 =  d_log(Dl4_r)\n",
    "    grad_r_w4_part_3 =   Dl3_rA\n",
    "    grad_r_w4 =     grad_r_w4_part_3.T.dot(grad_r_w4_part_1 * grad_r_w4_part_2) \n",
    "    grad_r_b4 = grad_r_w4_part_1 * grad_r_w4_part_2\n",
    "\n",
    "    grad_r_w3_part_1 =  (grad_r_w4_part_1 * grad_r_w4_part_2).dot(D_W4.T)\n",
    "    grad_r_w3_part_2 =  d_ReLu(Dl3_r)\n",
    "    grad_r_w3_part_3 =   Dl2_rA\n",
    "    grad_r_w3 =       grad_r_w3_part_3.T.dot(grad_r_w3_part_1 * grad_r_w3_part_2) \n",
    "    grad_r_b3 =      grad_r_w3_part_1 * grad_r_w3_part_2\n",
    "    \n",
    "    grad_r_w2_part_1 = (grad_r_w3_part_1 * grad_r_w3_part_2).dot(D_W3.T)\n",
    "    grad_r_w2_part_2 = d_ReLu(Dl2_r)\n",
    "    grad_r_w2_part_3 = Dl1_rA\n",
    "    grad_r_w2 = grad_r_w2_part_3.T.dot(grad_r_w2_part_1 * grad_r_w2_part_2)\n",
    "    grad_r_b2 = (grad_r_w2_part_1 * grad_r_w2_part_2)\n",
    "\n",
    "    grad_r_w1_part_1 = (grad_r_w2_part_1 * grad_r_w2_part_2).dot(D_W2.T)\n",
    "    grad_r_w1_part_2 = d_ReLu(Dl1_r)\n",
    "    grad_r_w1_part_3 = current_image\n",
    "    grad_r_w1 = grad_r_w1_part_3.T.dot(grad_r_w1_part_1 * grad_r_w1_part_2)\n",
    "    grad_r_b1 = grad_r_w1_part_1 * grad_r_w1_part_2\n",
    "\n",
    "    grad_w1 =grad_f_w1 + grad_r_w1\n",
    "    grad_w2 =grad_f_w2 + grad_r_w2\n",
    "    grad_w3 =grad_f_w3 + grad_r_w3\n",
    "    grad_w4 =grad_f_w4 + grad_r_w4\n",
    "\n",
    "    grad_b1 =grad_f_b1 + grad_r_b1\n",
    "    grad_b2 =grad_f_b2 + grad_r_b2\n",
    "    grad_b3 =grad_f_b3 + grad_r_b3\n",
    "    grad_b4 =grad_f_b4 + grad_r_b4\n",
    "\n",
    "    # ---- Update Gradient ----\n",
    "    m1w1 = beta_1 * m1w1 + (1 - beta_1) * grad_w1\n",
    "    v1w1 = beta_2 * v1w1 + (1 - beta_2) * grad_w1 ** 2\n",
    "    \n",
    "    m2w2 = beta_1 * m2w2 + (1 - beta_1) * grad_w2\n",
    "    v2w2 = beta_2 * v2w2 + (1 - beta_2) * grad_w2 ** 2\n",
    "    \n",
    "    m3w3 = beta_1 * m3w3 + (1 - beta_1) * grad_w3\n",
    "    v3w3 = beta_2 * v3w3 + (1 - beta_2) * grad_w3 ** 2\n",
    "    \n",
    "    m4w4 = beta_1 * m4w4 + (1 - beta_1) * grad_w4\n",
    "    v4w4 = beta_2 * v4w4 + (1 - beta_2) * grad_w4 ** 2\n",
    "    \n",
    "    m1b1 = beta_1 * m1b1 + (1 - beta_1) * grad_b1\n",
    "    v1b1 = beta_2 * v1b1 + (1 - beta_2) * grad_b1 ** 2\n",
    "    \n",
    "    m2b2 = beta_1 * m2b2 + (1 - beta_1) * grad_b2\n",
    "    v2b2 = beta_2 * v2b2 + (1 - beta_2) * grad_b2 ** 2\n",
    "    \n",
    "    m3b3 = beta_1 * m3b3 + (1 - beta_1) * grad_b3\n",
    "    v3b3 = beta_2 * v3b3 + (1 - beta_2) * grad_b3 ** 2\n",
    "    \n",
    "    m4b4 = beta_1 * m4b4 + (1 - beta_1) * grad_b4\n",
    "    v4b4 = beta_2 * v4b4 + (1 - beta_2) * grad_b4 ** 2\n",
    "\n",
    "    D_W1 = D_W1 - (learing_rate / (np.sqrt(v1w1 /(1-beta_2)**iter ) + eps)) * (m1v1/(1-beta_1)**iter)\n",
    "    D_b1 = D_b1 - (learing_rate / (np.sqrt(v1b1 /(1-beta_2)**iter ) + eps)) * (m1b1/(1-beta_1)**iter)\n",
    "        \n",
    "    D_W2 = D_W2 - (learing_rate / (np.sqrt(v2w2 /(1-beta_2)**iter ) + eps)) * (m2v2/(1-beta_1)**iter)\n",
    "    D_b2 = D_b2 - (learing_rate / (np.sqrt(v2b2 /(1-beta_2)**iter ) + eps)) * (m2b2/(1-beta_1)**iter)\n",
    "    \n",
    "    D_W3 = D_W3 - (learing_rate / (np.sqrt(v3w3 /(1-beta_2)**iter ) + eps)) * (m3v3/(1-beta_1)**iter)\n",
    "    D_b3 = D_b3 - (learing_rate / (np.sqrt(v3b3 /(1-beta_2)**iter ) + eps)) * (m3b3/(1-beta_1)**iter)\n",
    "    \n",
    "    D_W4 = D_W4 - (learing_rate / (np.sqrt(v4w4 /(1-beta_2)**iter ) + eps)) * (m4v4/(1-beta_1)**iter)\n",
    "    D_b4 = D_b4 - (learing_rate / (np.sqrt(v4b4 /(1-beta_2)**iter ) + eps)) * (m4b4/(1-beta_1)**iter)\n",
    "    \n",
    "    # Func: Forward Feed for G\n",
    "    Z = np.random.normal(-1., 1., size=[1, G_input]) \n",
    "    Gl1 = Z.dot(G_W1) + G_b1\n",
    "    Gl1A = Lrelu(Gl1)\n",
    "    Gl2 = Gl1A.dot(G_W2) + G_b2\n",
    "    Gl2A = Lrelu(Gl2)\n",
    "    Gl3 = Gl2A.dot(G_W3) + G_b3\n",
    "    Gl3A = Lrelu(Gl3)\n",
    "    Gl4 = Gl3A.dot(G_W4) + G_b4\n",
    "    Gl4A = Lrelu(Gl4)\n",
    "    Gl5 = Gl4A.dot(G_W5) + G_b5\n",
    "    Gl5A = Lrelu(Gl5)\n",
    "    Gl6 = Gl5A.dot(G_W6) + G_b6\n",
    "    Gl6A = Lrelu(Gl6)\n",
    "    Gl7 = Gl6A.dot(G_W7) + G_b7\n",
    "    Gl7A = Lrelu(Gl7)\n",
    "    Gl8 = Gl7A.dot(G_W8) + G_b8\n",
    "    Gl8A = Lrelu(Gl8)\n",
    "    Gl9 = Gl8A.dot(G_W9) + G_b9\n",
    "    Gl9A = Lrelu(Gl9)\n",
    "    Gl10 = Gl9A.dot(G_W10) + G_b10\n",
    "    Gl10A = Lrelu(Gl10)\n",
    "    Gl11 = Gl10A.dot(G_W11) + G_b11\n",
    "    \n",
    "    current_fake_data = log(Gl11)\n",
    "    #fig = plot(current_fake_data,\"FAKE\")\n",
    "    Dl1 = current_fake_data.dot(D_W1) + D_b1\n",
    "    Dl1_A = Lrelu(Dl1)\n",
    "    Dl2 = Dl1_A.dot(D_W2) + D_b2\n",
    "    Dl2_A = Lrelu(Dl2)\n",
    "    Dl3 = Dl2_A.dot(D_W3) + D_b3\n",
    "    Dl3_A = Lrelu(Dl3)\n",
    "    Dl4 = Dl3_A.dot(D_W4) + D_b4\n",
    "    Dl4_A = log(Dl4)\n",
    "\n",
    "    # Func: Cost G\n",
    "    G_cost = -np.log(Dl4_A)\n",
    "\n",
    "    # Func: Gradient\n",
    "    grad_G_w11_part_1 = (( (-1/Dl4_A) * d_log(Dl4).dot(D_W4.T) * (d_Lrelu(Dl3)) ).dot(D_W3.T).dot(D_W2.T).dot(D_W1.T))\n",
    "    grad_G_w11_part_2 = d_log(Gl11)\n",
    "    grad_G_w11_part_3 = Gl10A\n",
    "    grad_G_w11 = grad_G_w11_part_3.T.dot(grad_G_w11_part_1 * grad_G_w11_part_1)\n",
    "    grad_G_b11 = grad_G_w11_part_1 * grad_G_w11_part_2\n",
    "\n",
    "    grad_G_w10_part_1 = (grad_G_w11_part_1 * grad_G_w11_part_2).dot(G_W11.T)\n",
    "    grad_G_w10_part_2 = d_Lrelu(Gl10)\n",
    "    grad_G_w10_part_3 = Gl9A\n",
    "    grad_G_w10 = grad_G_w10_part_3.T.dot(grad_G_w10_part_1 * grad_G_w10_part_2)\n",
    "    grad_G_b10 = (grad_G_w10_part_1 * grad_G_w10_part_2)\n",
    "    \n",
    "    grad_G_w9_part_1 = (grad_G_w10_part_1 * grad_G_w10_part_2).dot(G_W10.T)\n",
    "    grad_G_w9_part_2 = d_Lrelu(Gl9)\n",
    "    grad_G_w9_part_3 = Gl8A\n",
    "    grad_G_w9 = grad_G_w9_part_3.T.dot(grad_G_w9_part_1 * grad_G_w9_part_2)\n",
    "    grad_G_b9 = (grad_G_w9_part_1 * grad_G_w9_part_2)\n",
    "    \n",
    "    grad_G_w8_part_1 = (grad_G_w9_part_1 * grad_G_w9_part_2).dot(G_W9.T)\n",
    "    grad_G_w8_part_2 = d_Lrelu(Gl8)\n",
    "    grad_G_w8_part_3 = Gl7A\n",
    "    grad_G_w8 = grad_G_w8_part_3.T.dot(grad_G_w8_part_1 * grad_G_w8_part_2)\n",
    "    grad_G_b8 = (grad_G_w8_part_1 * grad_G_w8_part_2)\n",
    "   \n",
    "    grad_G_w7_part_1 = (grad_G_w8_part_1 * grad_G_w8_part_2).dot(G_W8.T)\n",
    "    grad_G_w7_part_2 = d_Lrelu(Gl7)\n",
    "    grad_G_w7_part_3 = Gl6A\n",
    "    grad_G_w7 = grad_G_w7_part_3.T.dot(grad_G_w7_part_1 * grad_G_w7_part_2)\n",
    "    grad_G_b7 = (grad_G_w7_part_1 * grad_G_w7_part_2)\n",
    "\n",
    "    grad_G_w6_part_1 = (grad_G_w7_part_1 * grad_G_w7_part_2).dot(G_W7.T)\n",
    "    grad_G_w6_part_2 = d_Lrelu(Gl6)\n",
    "    grad_G_w6_part_3 = Gl5A\n",
    "    grad_G_w6 = grad_G_w6_part_3.T.dot(grad_G_w6_part_1 * grad_G_w6_part_2)\n",
    "    grad_G_b6 = (grad_G_w6_part_1 * grad_G_w6_part_2)\n",
    "\n",
    "    grad_G_w5_part_1 = (grad_G_w6_part_1 * grad_G_w6_part_2).dot(G_W6.T)\n",
    "    grad_G_w5_part_2 = d_Lrelu(Gl5)\n",
    "    grad_G_w5_part_3 = Gl4A\n",
    "    grad_G_w5 = grad_G_w5_part_3.T.dot(grad_G_w5_part_1 * grad_G_w5_part_2)\n",
    "    grad_G_b5 = (grad_G_w5_part_1 * grad_G_w5_part_2)\n",
    "\n",
    "    grad_G_w4_part_1 = (grad_G_w5_part_1 * grad_G_w5_part_2).dot(G_W5.T)\n",
    "    grad_G_w4_part_2 = d_Lrelu(Gl4)\n",
    "    grad_G_w4_part_3 = Gl3A\n",
    "    grad_G_w4 = grad_G_w4_part_3.T.dot(grad_G_w4_part_1 * grad_G_w4_part_2)\n",
    "    grad_G_b4 = (grad_G_w4_part_1 * grad_G_w4_part_2)\n",
    "\n",
    "    grad_G_w3_part_1 = (grad_G_w4_part_1 * grad_G_w4_part_2).dot(G_W4.T)\n",
    "    grad_G_w3_part_2 = d_Lrelu(Gl3)\n",
    "    grad_G_w3_part_3 = Gl2A\n",
    "    grad_G_w3 = grad_G_w3_part_3.T.dot(grad_G_w3_part_1 * grad_G_w3_part_2)\n",
    "    grad_G_b3 = (grad_G_w3_part_1 * grad_G_w3_part_2)\n",
    "\n",
    "    grad_G_w2_part_1 = (grad_G_w3_part_1 * grad_G_w3_part_2).dot(G_W3.T)\n",
    "    grad_G_w2_part_2 = d_Lrelu(Gl2)\n",
    "    grad_G_w2_part_3 = Gl1A\n",
    "    grad_G_w2 = grad_G_w2_part_3.T.dot(grad_G_w2_part_1 * grad_G_w2_part_2)\n",
    "    grad_G_b2 = (grad_G_w2_part_1 * grad_G_w2_part_2)\n",
    "\n",
    "    grad_G_w1_part_1 = (grad_G_w2_part_1 * grad_G_w2_part_2).dot(G_W2.T)\n",
    "    grad_G_w1_part_2 = d_Lrelu(Gl1)\n",
    "    grad_G_w1_part_3 = Z\n",
    "    grad_G_w1 = grad_G_w1_part_3.T.dot(grad_G_w1_part_1 * grad_G_w1_part_2)\n",
    "    grad_G_b1 = grad_G_w1_part_1 * grad_G_w1_part_2\n",
    "\n",
    "    # ---- Update Gradient ----\n",
    "    m5 = beta_1 * m5 + (1 - beta_1) * grad_G_w1\n",
    "    v5 = beta_2 * v5 + (1 - beta_2) * grad_G_w1 ** 2\n",
    "\n",
    "    m6 = beta_1 * m6 + (1 - beta_1) * grad_G_b1\n",
    "    v6 = beta_2 * v6 + (1 - beta_2) * grad_G_b1 ** 2\n",
    "\n",
    "    m7 = beta_1 * m7 + (1 - beta_1) * grad_G_w2\n",
    "    v7 = beta_2 * v7 + (1 - beta_2) * grad_G_w2 ** 2\n",
    "\n",
    "    m8 = beta_1 * m8 + (1 - beta_1) * grad_G_b2\n",
    "    v8 = beta_2 * v8 + (1 - beta_2) * grad_G_b2 ** 2\n",
    "\n",
    "    m9 = beta_1 * m9 + (1 - beta_1) * grad_G_w3\n",
    "    v9 = beta_2 * v9 + (1 - beta_2) * grad_G_w3 ** 2\n",
    "\n",
    "    m10 = beta_1 * m10 + (1 - beta_1) * grad_G_b3\n",
    "    v10 = beta_2 * v10 + (1 - beta_2) * grad_G_b3 ** 2\n",
    "\n",
    "    m11 = beta_1 * m11 + (1 - beta_1) * grad_G_w4\n",
    "    v11 = beta_2 * v11 + (1 - beta_2) * grad_G_w4 ** 2\n",
    "\n",
    "    m12 = beta_1 * m12 + (1 - beta_1) * grad_G_b4\n",
    "    v12 = beta_2 * v12 + (1 - beta_2) * grad_G_b4 ** 2\n",
    "\n",
    "    m13 = beta_1 * m13 + (1 - beta_1) * grad_G_w5\n",
    "    v13 = beta_2 * v13 + (1 - beta_2) * grad_G_w5 ** 2\n",
    "\n",
    "    m14 = beta_1 * m14 + (1 - beta_1) * grad_G_b5\n",
    "    v14 = beta_2 * v14 + (1 - beta_2) * grad_G_b5 ** 2\n",
    "\n",
    "    m15 = beta_1 * m15 + (1 - beta_1) * grad_G_w6\n",
    "    v15 = beta_2 * v15 + (1 - beta_2) * grad_G_w6 ** 2\n",
    "\n",
    "    m16 = beta_1 * m16 + (1 - beta_1) * grad_G_b6\n",
    "    v16 = beta_2 * v16 + (1 - beta_2) * grad_G_b6 ** 2\n",
    "\n",
    "    m17 = beta_1 * m17 + (1 - beta_1) * grad_G_w7\n",
    "    v17 = beta_2 * v17 + (1 - beta_2) * grad_G_w7 ** 2\n",
    "\n",
    "    m18 = beta_1 * m18 + (1 - beta_1) * grad_G_b7\n",
    "    v18 = beta_2 * v18 + (1 - beta_2) * grad_G_b7 ** 2\n",
    "    \n",
    "    m19 = beta_1 * m19 + (1 - beta_1) * grad_G_w8\n",
    "    v19 = beta_2 * v19 + (1 - beta_2) * grad_G_w8 ** 2\n",
    "    \n",
    "    m20 = beta_1 * m20 + (1 - beta_1) * grad_G_b8\n",
    "    v20 = beta_2 * v20 + (1 - beta_2) * grad_G_b8 ** 2\n",
    "    \n",
    "    m21 = beta_1 * m21 + (1 - beta_1) * grad_G_w9\n",
    "    v21 = beta_2 * v21 + (1 - beta_2) * grad_G_w9 ** 2\n",
    "    \n",
    "    m22 = beta_1 * m22 + (1 - beta_1) * grad_G_b9\n",
    "    v22 = beta_2 * v22 + (1 - beta_2) * grad_G_b9 ** 2\n",
    "    \n",
    "    m23 = beta_1 * m23 + (1 - beta_1) * grad_G_w10\n",
    "    v23 = beta_2 * v23 + (1 - beta_2) * grad_G_w10 ** 2\n",
    "\n",
    "    m24 = beta_1 * m24 + (1 - beta_1) * grad_G_b10\n",
    "    v24 = beta_2 * v24 + (1 - beta_2) * grad_G_b10 ** 2\n",
    "    \n",
    "    m25 = beta_1 * m25 + (1 - beta_1) * grad_G_w11\n",
    "    v25 = beta_2 * v25 + (1 - beta_2) * grad_G_w11 ** 2\n",
    "    \n",
    "    m26 = beta_1 * m26 + (1 - beta_1) * grad_G_b11\n",
    "    v26 = beta_2 * v26 + (1 - beta_2) * grad_G_b11 ** 2\n",
    "    \n",
    "\n",
    "    G_W1 = G_W1 - (learing_rate / (np.sqrt(v5 /(1-beta_2)**iter ) + eps)) * (m5/(1-beta_1)**iter)\n",
    "    G_b1 = G_b1 - (learing_rate / (np.sqrt(v6 /(1-beta_2)**iter ) + eps)) * (m6/(1-beta_1)**iter)\n",
    "    \n",
    "    G_W2 = G_W2 - (learing_rate / (np.sqrt(v7 /(1-beta_2)**iter ) + eps)) * (m7/(1-beta_1)**iter)\n",
    "    G_b2 = G_b2 - (learing_rate / (np.sqrt(v8 /(1-beta_2)**iter ) + eps)) * (m8/(1-beta_1)**iter)\n",
    "\n",
    "    G_W3 = G_W3 - (learing_rate / (np.sqrt(v9 /(1-beta_2)**iter ) + eps)) * (m9/(1-beta_1)**iter)\n",
    "    G_b3 = G_b3 - (learing_rate / (np.sqrt(v10 /(1-beta_2)**iter ) + eps)) * (m10/(1-beta_1)**iter)\n",
    "\n",
    "    G_W4 = G_W4 - (learing_rate / (np.sqrt(v11 /(1-beta_2)**iter ) + eps)) * (m11/(1-beta_1)**iter)\n",
    "    G_b4 = G_b4 - (learing_rate / (np.sqrt(v12 /(1-beta_2)**iter ) + eps)) * (m12/(1-beta_1)**iter)\n",
    "\n",
    "    G_W5 = G_W5 - (learing_rate / (np.sqrt(v13 /(1-beta_2)**iter ) + eps)) * (m13/(1-beta_1)**iter)\n",
    "    G_b5 = G_b5 - (learing_rate / (np.sqrt(v14 /(1-beta_2)**iter ) + eps)) * (m14/(1-beta_1)**iter)\n",
    "\n",
    "    G_W6 = G_W6 - (learing_rate / (np.sqrt(v15 /(1-beta_2)**iter ) + eps)) * (m15/(1-beta_1)**iter)\n",
    "    G_b6 = G_b6 - (learing_rate / (np.sqrt(v16 /(1-beta_2)**iter ) + eps)) * (m16/(1-beta_1)**iter)\n",
    "\n",
    "    G_W7 = G_W7 - (learing_rate / (np.sqrt(v17 /(1-beta_2)**iter ) + eps)) * (m17/(1-beta_1)**iter)\n",
    "    G_b7 = G_b7 - (learing_rate / (np.sqrt(v18 /(1-beta_2)**iter ) + eps)) * (m18/(1-beta_1)**iter)\n",
    "    \n",
    "    G_W8 = G_W8 - (learing_rate / (np.sqrt(v19 /(1-beta_2)**iter ) + eps)) * (m19/(1-beta_1)**iter)\n",
    "    G_b8 = G_b8 - (learing_rate / (np.sqrt(v20 /(1-beta_2)**iter ) + eps)) * (m20/(1-beta_1)**iter)\n",
    "    \n",
    "    G_W9 = G_W9 - (learing_rate / (np.sqrt(v21 /(1-beta_2)**iter ) + eps)) * (m21/(1-beta_1)**iter)\n",
    "    G_b9 = G_b9 - (learing_rate / (np.sqrt(v22 /(1-beta_2)**iter ) + eps)) * (m22/(1-beta_1)**iter)\n",
    "    \n",
    "    G_W10 = G_W10 - (learing_rate / (np.sqrt(v23 /(1-beta_2)**iter ) + eps)) * (m23/(1-beta_1)**iter)\n",
    "    G_b10 = G_b10 - (learing_rate / (np.sqrt(v24 /(1-beta_2)**iter ) + eps)) * (m24/(1-beta_1)**iter)\n",
    "    \n",
    "    G_W11 = G_W11 - (learing_rate / (np.sqrt(v25 /(1-beta_2)**iter ) + eps)) * (m25/(1-beta_1)**iter)\n",
    "    G_b11 = G_b11 - (learing_rate / (np.sqrt(v26 /(1-beta_2)**iter ) + eps)) * (m26/(1-beta_1)**iter)\n",
    "\n",
    "    print(\"Current Iter: \",iter, \" Current D cost:\",D_cost, \" Current G cost: \", G_cost)\n",
    "    \n",
    "    if iter == 0:\n",
    "        learing_rate = learing_rate * 0.01\n",
    "    if iter == 40:\n",
    "        learing_rate = learing_rate * 0.01\n",
    "\n",
    "    # ---- Print to Out put ----\n",
    "# -- end code --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "l=[]\n",
    "for i in range(10):\n",
    "    Z = np.random.uniform(-1., 1., size=[1, G_input])\n",
    "    Gl1 = Z.dot(G_W1) + G_b1\n",
    "    Gl1A = Lrelu(Gl1)\n",
    "    Gl2 = Gl1A.dot(G_W2) + G_b2\n",
    "    Gl2A = Lrelu(Gl2)\n",
    "    Gl3 = Gl2A.dot(G_W3) + G_b3\n",
    "    Gl3A = Lrelu(Gl3)\n",
    "    Gl4 = Gl3A.dot(G_W4) + G_b4\n",
    "    Gl4A = Lrelu(Gl4)\n",
    "    Gl5 = Gl4A.dot(G_W5) + G_b5\n",
    "    Gl5A = Lrelu(Gl5)\n",
    "    Gl6 = Gl5A.dot(G_W6) + G_b6\n",
    "    Gl6A = Lrelu(Gl6)\n",
    "    Gl7 = Gl6A.dot(G_W7) + G_b7\n",
    "    Gl7A = Lrelu(Gl7)\n",
    "    Gl8 = Gl7A.dot(G_W8) + G_b8\n",
    "    Gl8A = Lrelu(Gl8)\n",
    "    Gl9 = Gl8A.dot(G_W9) + G_b9\n",
    "    Gl9A = Lrelu(Gl9)\n",
    "    Gl10 = Gl9A.dot(G_W10) + G_b10\n",
    "    Gl10A = Lrelu(Gl10)\n",
    "    Gl11 = Gl10A.dot(G_W11) + G_b11\n",
    "\n",
    "    current_fake_data = log(Gl11)\n",
    "    l.append(current_fake_data)\n",
    "fig = plot(l,\"TEST\")\n",
    "fig.savefig('Click_Me_{}.png'.format(str(iter).zfill(3)+\"_Ginput_\"+str(G_input)+ \"_hiddenone\"+str(hidden_input) + \"_hiddentwo\"+str(hidden_input2) + \"_LR_\" + str(learing_rate)), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "grad_f_w1_part_1 = (grad_f_w2_part_1 * grad_f_w2_part_2).dot(D_W2.T)\n",
    "grad_f_w1_part_2 = d_ReLu(Dl1_f)\n",
    "grad_f_w1_part_3 = current_fake_data\n",
    "grad_f_w1 = grad_f_w1_part_3.T.dot(grad_f_w1_part_1 * grad_f_w1_part_2)\n",
    "grad_f_b1 = grad_f_w1_part_1 * grad_f_w1_part_2\n",
    "\n",
    "print(len(grad_f_w1_part_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1400\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(grad_r_w1_part_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1400\n"
     ]
    }
   ],
   "source": [
    "print(len(current_fake_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = np.random.normal(-1., 1., size=[1, G_input]) \n",
    "Gl1 = Z.dot(G_W1) + G_b1\n",
    "Gl1A = Lrelu(Gl1)\n",
    "Gl2 = Gl1A.dot(G_W2) + G_b2\n",
    "Gl2A = Lrelu(Gl2)\n",
    "Gl3 = Gl2A.dot(G_W3) + G_b3\n",
    "Gl3A = Lrelu(Gl3)\n",
    "Gl4 = Gl3A.dot(G_W4) + G_b4\n",
    "Gl4A = Lrelu(Gl4)\n",
    "Gl5 = Gl4A.dot(G_W5) + G_b5\n",
    "Gl5A = Lrelu(Gl5)\n",
    "Gl6 = Gl5A.dot(G_W6) + G_b6\n",
    "Gl6A = Lrelu(Gl6)\n",
    "Gl7 = Gl6A.dot(G_W7) + G_b7\n",
    "Gl7A = Lrelu(Gl7)\n",
    "Gl8 = Gl7A.dot(G_W8) + G_b8\n",
    "Gl8A = Lrelu(Gl8)\n",
    "Gl9 = Gl8A.dot(G_W9) + G_b9\n",
    "Gl9A = Lrelu(Gl9)\n",
    "Gl10 = Gl9A.dot(G_W10) + G_b10\n",
    "Gl10A = Lrelu(Gl10)\n",
    "Gl11 = Gl10A.dot(G_W11) + G_b11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gl10 = Gl9A.dot(G_W10) + G_b10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1400"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Gl9A.dot(G_W10)+ G_b10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
